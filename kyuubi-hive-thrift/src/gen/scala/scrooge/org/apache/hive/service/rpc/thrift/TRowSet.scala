/**
 * Generated by Scrooge
 *   version: 19.12.0
 *   rev: dfdb68cf6b9c501dbbe3ae644504bf403ad76bfa
 *   built at: 20191212-171820
 */
package org.apache.hive.service.rpc.thrift

import com.twitter.io.Buf
import com.twitter.scrooge.{
  InvalidFieldsException,
  LazyTProtocol,
  StructBuilder,
  StructBuilderFactory,
  TFieldBlob,
  ThriftStruct,
  ThriftStructCodec3,
  ThriftStructField,
  ThriftStructFieldInfo,
  ThriftStructMetaData,
  ValidatingThriftStruct,
  ValidatingThriftStructCodec3
}
import org.apache.thrift.protocol._
import org.apache.thrift.transport.TMemoryBuffer
import scala.collection.immutable.{Map => immutable$Map}
import scala.collection.mutable.Builder
import scala.reflect.{ClassTag, classTag}


object TRowSet extends ValidatingThriftStructCodec3[TRowSet] with StructBuilderFactory[TRowSet] {
  val NoPassthroughFields: immutable$Map[Short, TFieldBlob] = immutable$Map.empty[Short, TFieldBlob]
  val Struct: TStruct = new TStruct("TRowSet")
  val StartRowOffsetField: TField = new TField("startRowOffset", TType.I64, 1)
  val StartRowOffsetFieldManifest: Manifest[Long] = implicitly[Manifest[Long]]
  val RowsField: TField = new TField("rows", TType.LIST, 2)
  val RowsFieldManifest: Manifest[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow]] = implicitly[Manifest[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow]]]
  val ColumnsField: TField = new TField("columns", TType.LIST, 3)
  val ColumnsFieldManifest: Manifest[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]] = implicitly[Manifest[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]]]
  val BinaryColumnsField: TField = new TField("binaryColumns", TType.STRING, 4)
  val BinaryColumnsFieldManifest: Manifest[_root_.java.nio.ByteBuffer] = implicitly[Manifest[_root_.java.nio.ByteBuffer]]
  val ColumnCountField: TField = new TField("columnCount", TType.I32, 5)
  val ColumnCountFieldManifest: Manifest[Int] = implicitly[Manifest[Int]]

  /**
   * Field information in declaration order.
   */
  lazy val fieldInfos: scala.List[ThriftStructFieldInfo] = scala.List[ThriftStructFieldInfo](
    new ThriftStructFieldInfo(
      StartRowOffsetField,
      false,
      true,
      StartRowOffsetFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      RowsField,
      false,
      true,
      RowsFieldManifest,
      _root_.scala.None,
      _root_.scala.Some(implicitly[Manifest[org.apache.hive.service.rpc.thrift.TRow]]),
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      ColumnsField,
      true,
      false,
      ColumnsFieldManifest,
      _root_.scala.None,
      _root_.scala.Some(implicitly[Manifest[org.apache.hive.service.rpc.thrift.TColumn]]),
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      BinaryColumnsField,
      true,
      false,
      BinaryColumnsFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    ),
    new ThriftStructFieldInfo(
      ColumnCountField,
      true,
      false,
      ColumnCountFieldManifest,
      _root_.scala.None,
      _root_.scala.None,
      immutable$Map.empty[String, String],
      immutable$Map.empty[String, String],
      None
    )
  )

  lazy val structAnnotations: immutable$Map[String, String] =
    immutable$Map.empty[String, String]

  private val fieldTypes: IndexedSeq[ClassTag[_]] = IndexedSeq(
    classTag[Long].asInstanceOf[ClassTag[_]],
    classTag[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow]].asInstanceOf[ClassTag[_]],
    classTag[_root_.scala.Option[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]]].asInstanceOf[ClassTag[_]],
    classTag[_root_.scala.Option[_root_.java.nio.ByteBuffer]].asInstanceOf[ClassTag[_]],
    classTag[_root_.scala.Option[Int]].asInstanceOf[ClassTag[_]]
  )

  private[this] val structFields: Seq[ThriftStructField[TRowSet]] = {
    Seq(
      new ThriftStructField[TRowSet](
        StartRowOffsetField,
        _root_.scala.Some(StartRowOffsetFieldManifest),
        classOf[TRowSet]) {
          def getValue[R](struct: TRowSet): R = struct.startRowOffset.asInstanceOf[R]
      },
      new ThriftStructField[TRowSet](
        RowsField,
        _root_.scala.Some(RowsFieldManifest),
        classOf[TRowSet]) {
          def getValue[R](struct: TRowSet): R = struct.rows.asInstanceOf[R]
      },
      new ThriftStructField[TRowSet](
        ColumnsField,
        _root_.scala.Some(ColumnsFieldManifest),
        classOf[TRowSet]) {
          def getValue[R](struct: TRowSet): R = struct.columns.asInstanceOf[R]
      },
      new ThriftStructField[TRowSet](
        BinaryColumnsField,
        _root_.scala.Some(BinaryColumnsFieldManifest),
        classOf[TRowSet]) {
          def getValue[R](struct: TRowSet): R = struct.binaryColumns.asInstanceOf[R]
      },
      new ThriftStructField[TRowSet](
        ColumnCountField,
        _root_.scala.Some(ColumnCountFieldManifest),
        classOf[TRowSet]) {
          def getValue[R](struct: TRowSet): R = struct.columnCount.asInstanceOf[R]
      }
    )
  }

  override lazy val metaData: ThriftStructMetaData[TRowSet] =
    new ThriftStructMetaData(this, structFields, fieldInfos, Seq(), structAnnotations)

  /**
   * Checks that all required fields are non-null.
   */
  def validate(_item: TRowSet): Unit = {
    if (_item.rows == null) throw new TProtocolException("Required field rows cannot be null")
  }

  /**
   * Checks that the struct is a valid as a new instance. If there are any missing required or
   * construction required fields, return a non-empty list.
   */
  def validateNewInstance(item: TRowSet): scala.Seq[com.twitter.scrooge.validation.Issue] = {
    val buf = scala.collection.mutable.ListBuffer.empty[com.twitter.scrooge.validation.Issue]

    buf ++= validateField(item.startRowOffset)
    if (item.rows == null)
      buf += com.twitter.scrooge.validation.MissingRequiredField(fieldInfos.apply(1))
    buf ++= validateField(item.rows)
    buf ++= validateField(item.columns)
    buf ++= validateField(item.binaryColumns)
    buf ++= validateField(item.columnCount)
    buf.toList
  }

  def withoutPassthroughFields(original: TRowSet): TRowSet =
    new Immutable(
      startRowOffset =
        {
          val field = original.startRowOffset
          field
        },
      rows =
        {
          val field = original.rows
          field.map { field =>
            org.apache.hive.service.rpc.thrift.TRow.withoutPassthroughFields(field)
          }
        },
      columns =
        {
          val field = original.columns
          field.map { field =>
            field.map { field =>
              org.apache.hive.service.rpc.thrift.TColumn.withoutPassthroughFields(field)
            }
          }
        },
      binaryColumns =
        {
          val field = original.binaryColumns
          field.map { field =>
            field
          }
        },
      columnCount =
        {
          val field = original.columnCount
          field.map { field =>
            field
          }
        }
    )

  def newBuilder(): StructBuilder[TRowSet] = new TRowSetStructBuilder(_root_.scala.None, fieldTypes)

  override def encode(_item: TRowSet, _oproto: TProtocol): Unit = {
    _item.write(_oproto)
  }


  private[this] def lazyDecode(_iprot: LazyTProtocol): TRowSet = {

    var startRowOffset: Long = 0L
    var _got_startRowOffset = false
    var rows: _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow] = _root_.scala.collection.immutable.Nil
    var _got_rows = false
    var columns: Option[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]] = None
    var binaryColumns: Option[_root_.java.nio.ByteBuffer] = None
    var columnCountOffset: Int = -1

    var _passthroughFields: Builder[(Short, TFieldBlob), immutable$Map[Short, TFieldBlob]] = null
    var _done = false
    val _start_offset = _iprot.offset

    _iprot.readStructBegin()
    while (!_done) {
      val _field = _iprot.readFieldBegin()
      if (_field.`type` == TType.STOP) {
        _done = true
      } else {
        _field.id match {
          case 1 =>
            _field.`type` match {
              case TType.I64 =>
    
                startRowOffset = readStartRowOffsetValue(_iprot)
                _got_startRowOffset = true
              case _actualType =>
                val _expectedType = TType.I64
                throw new TProtocolException(
                  "Received wrong type for field 'startRowOffset' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 2 =>
            _field.`type` match {
              case TType.LIST =>
    
                rows = readRowsValue(_iprot)
                _got_rows = true
              case _actualType =>
                val _expectedType = TType.LIST
                throw new TProtocolException(
                  "Received wrong type for field 'rows' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 3 =>
            _field.`type` match {
              case TType.LIST =>
    
                columns = Some(readColumnsValue(_iprot))
              case _actualType =>
                val _expectedType = TType.LIST
                throw new TProtocolException(
                  "Received wrong type for field 'columns' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 4 =>
            _field.`type` match {
              case TType.STRING =>
    
                binaryColumns = Some(readBinaryColumnsValue(_iprot))
              case _actualType =>
                val _expectedType = TType.STRING
                throw new TProtocolException(
                  "Received wrong type for field 'binaryColumns' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 5 =>
            _field.`type` match {
              case TType.I32 =>
                columnCountOffset = _iprot.offsetSkipI32
    
              case _actualType =>
                val _expectedType = TType.I32
                throw new TProtocolException(
                  "Received wrong type for field 'columnCount' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case _ =>
            if (_passthroughFields == null)
              _passthroughFields = immutable$Map.newBuilder[Short, TFieldBlob]
            _passthroughFields += (_field.id -> TFieldBlob.read(_field, _iprot))
        }
        _iprot.readFieldEnd()
      }
    }
    _iprot.readStructEnd()

    if (!_got_startRowOffset) throw new TProtocolException("Required field 'startRowOffset' was not found in serialized data for struct TRowSet")
    if (!_got_rows) throw new TProtocolException("Required field 'rows' was not found in serialized data for struct TRowSet")
    new LazyImmutable(
      _iprot,
      _iprot.buffer,
      _start_offset,
      _iprot.offset,
      startRowOffset,
      rows,
      columns,
      binaryColumns,
      columnCountOffset,
      if (_passthroughFields == null)
        NoPassthroughFields
      else
        _passthroughFields.result()
    )
  }

  override def decode(_iprot: TProtocol): TRowSet =
    _iprot match {
      case i: LazyTProtocol => lazyDecode(i)
      case i => eagerDecode(i)
    }

  private[thrift] def eagerDecode(_iprot: TProtocol): TRowSet = {
    var startRowOffset: Long = 0L
    var _got_startRowOffset = false
    var rows: _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow] = _root_.scala.collection.immutable.Nil
    var _got_rows = false
    var columns: _root_.scala.Option[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]] = _root_.scala.None
    var binaryColumns: _root_.scala.Option[_root_.java.nio.ByteBuffer] = _root_.scala.None
    var columnCount: _root_.scala.Option[Int] = _root_.scala.None
    var _passthroughFields: Builder[(Short, TFieldBlob), immutable$Map[Short, TFieldBlob]] = null
    var _done = false

    _iprot.readStructBegin()
    while (!_done) {
      val _field = _iprot.readFieldBegin()
      if (_field.`type` == TType.STOP) {
        _done = true
      } else {
        _field.id match {
          case 1 =>
            _field.`type` match {
              case TType.I64 =>
                startRowOffset = readStartRowOffsetValue(_iprot)
                _got_startRowOffset = true
              case _actualType =>
                val _expectedType = TType.I64
                throw new TProtocolException(
                  "Received wrong type for field 'startRowOffset' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 2 =>
            _field.`type` match {
              case TType.LIST =>
                rows = readRowsValue(_iprot)
                _got_rows = true
              case _actualType =>
                val _expectedType = TType.LIST
                throw new TProtocolException(
                  "Received wrong type for field 'rows' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 3 =>
            _field.`type` match {
              case TType.LIST =>
                columns = _root_.scala.Some(readColumnsValue(_iprot))
              case _actualType =>
                val _expectedType = TType.LIST
                throw new TProtocolException(
                  "Received wrong type for field 'columns' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 4 =>
            _field.`type` match {
              case TType.STRING =>
                binaryColumns = _root_.scala.Some(readBinaryColumnsValue(_iprot))
              case _actualType =>
                val _expectedType = TType.STRING
                throw new TProtocolException(
                  "Received wrong type for field 'binaryColumns' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case 5 =>
            _field.`type` match {
              case TType.I32 =>
                columnCount = _root_.scala.Some(readColumnCountValue(_iprot))
              case _actualType =>
                val _expectedType = TType.I32
                throw new TProtocolException(
                  "Received wrong type for field 'columnCount' (expected=%s, actual=%s).".format(
                    ttypeToString(_expectedType),
                    ttypeToString(_actualType)
                  )
                )
            }
          case _ =>
            if (_passthroughFields == null)
              _passthroughFields = immutable$Map.newBuilder[Short, TFieldBlob]
            _passthroughFields += (_field.id -> TFieldBlob.read(_field, _iprot))
        }
        _iprot.readFieldEnd()
      }
    }
    _iprot.readStructEnd()

    if (!_got_startRowOffset) throw new TProtocolException("Required field 'startRowOffset' was not found in serialized data for struct TRowSet")
    if (!_got_rows) throw new TProtocolException("Required field 'rows' was not found in serialized data for struct TRowSet")
    new Immutable(
      startRowOffset,
      rows,
      columns,
      binaryColumns,
      columnCount,
      if (_passthroughFields == null)
        NoPassthroughFields
      else
        _passthroughFields.result()
    )
  }

  def apply(
    startRowOffset: Long,
    rows: _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow] = _root_.scala.collection.immutable.Nil,
    columns: _root_.scala.Option[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]] = _root_.scala.None,
    binaryColumns: _root_.scala.Option[_root_.java.nio.ByteBuffer] = _root_.scala.None,
    columnCount: _root_.scala.Option[Int] = _root_.scala.None
  ): TRowSet =
    new Immutable(
      startRowOffset,
      rows,
      columns,
      binaryColumns,
      columnCount
    )

  def unapply(_item: TRowSet): _root_.scala.Option[_root_.scala.Tuple5[Long, _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow], Option[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]], Option[_root_.java.nio.ByteBuffer], Option[Int]]] = _root_.scala.Some(_item.toTuple)


  @inline private[thrift] def readStartRowOffsetValue(_iprot: TProtocol): Long = {
    _iprot.readI64()
  }

  @inline private def writeStartRowOffsetField(startRowOffset_item: Long, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(StartRowOffsetField)
    writeStartRowOffsetValue(startRowOffset_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeStartRowOffsetValue(startRowOffset_item: Long, _oprot: TProtocol): Unit = {
    _oprot.writeI64(startRowOffset_item)
  }

  @inline private[thrift] def readRowsValue(_iprot: TProtocol): _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow] = {
    val _list = _iprot.readListBegin()
    if (_list.size == 0) {
      _iprot.readListEnd()
      Nil
    } else {
      val _rv = new _root_.scala.collection.mutable.ArrayBuffer[org.apache.hive.service.rpc.thrift.TRow](_list.size)
      var _i = 0
      while (_i < _list.size) {
        _rv += {
          org.apache.hive.service.rpc.thrift.TRow.decode(_iprot)
        }
        _i += 1
      }
      _iprot.readListEnd()
      _rv
    }
  }

  @inline private def writeRowsField(rows_item: _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow], _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(RowsField)
    writeRowsValue(rows_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeRowsValue(rows_item: _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow], _oprot: TProtocol): Unit = {
    _oprot.writeListBegin(new TList(TType.STRUCT, rows_item.size))
    rows_item match {
      case _: IndexedSeq[_] =>
        var _i = 0
        val _size = rows_item.size
        while (_i < _size) {
          val rows_item_element = rows_item(_i)
          rows_item_element.write(_oprot)
          _i += 1
        }
      case _ =>
        rows_item.foreach { rows_item_element =>
          rows_item_element.write(_oprot)
        }
    }
    _oprot.writeListEnd()
  }

  @inline private[thrift] def readColumnsValue(_iprot: TProtocol): _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn] = {
    val _list = _iprot.readListBegin()
    if (_list.size == 0) {
      _iprot.readListEnd()
      Nil
    } else {
      val _rv = new _root_.scala.collection.mutable.ArrayBuffer[org.apache.hive.service.rpc.thrift.TColumn](_list.size)
      var _i = 0
      while (_i < _list.size) {
        _rv += {
          org.apache.hive.service.rpc.thrift.TColumn.decode(_iprot)
        }
        _i += 1
      }
      _iprot.readListEnd()
      _rv
    }
  }

  @inline private def writeColumnsField(columns_item: _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn], _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(ColumnsField)
    writeColumnsValue(columns_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeColumnsValue(columns_item: _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn], _oprot: TProtocol): Unit = {
    _oprot.writeListBegin(new TList(TType.STRUCT, columns_item.size))
    columns_item match {
      case _: IndexedSeq[_] =>
        var _i = 0
        val _size = columns_item.size
        while (_i < _size) {
          val columns_item_element = columns_item(_i)
          columns_item_element.write(_oprot)
          _i += 1
        }
      case _ =>
        columns_item.foreach { columns_item_element =>
          columns_item_element.write(_oprot)
        }
    }
    _oprot.writeListEnd()
  }

  @inline private[thrift] def readBinaryColumnsValue(_iprot: TProtocol): _root_.java.nio.ByteBuffer = {
    _iprot.readBinary()
  }

  @inline private def writeBinaryColumnsField(binaryColumns_item: _root_.java.nio.ByteBuffer, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(BinaryColumnsField)
    writeBinaryColumnsValue(binaryColumns_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeBinaryColumnsValue(binaryColumns_item: _root_.java.nio.ByteBuffer, _oprot: TProtocol): Unit = {
    _oprot.writeBinary(binaryColumns_item)
  }

  @inline private[thrift] def readColumnCountValue(_iprot: TProtocol): Int = {
    _iprot.readI32()
  }

  @inline private def writeColumnCountField(columnCount_item: Int, _oprot: TProtocol): Unit = {
    _oprot.writeFieldBegin(ColumnCountField)
    writeColumnCountValue(columnCount_item, _oprot)
    _oprot.writeFieldEnd()
  }

  @inline private def writeColumnCountValue(columnCount_item: Int, _oprot: TProtocol): Unit = {
    _oprot.writeI32(columnCount_item)
  }


  object Immutable extends ThriftStructCodec3[TRowSet] {
    override def encode(_item: TRowSet, _oproto: TProtocol): Unit = { _item.write(_oproto) }
    override def decode(_iprot: TProtocol): TRowSet = TRowSet.decode(_iprot)
    override lazy val metaData: ThriftStructMetaData[TRowSet] = TRowSet.metaData
  }

  /**
   * The default read-only implementation of TRowSet.  You typically should not need to
   * directly reference this class; instead, use the TRowSet.apply method to construct
   * new instances.
   */
  class Immutable(
      val startRowOffset: Long,
      val rows: _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow],
      val columns: _root_.scala.Option[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]],
      val binaryColumns: _root_.scala.Option[_root_.java.nio.ByteBuffer],
      val columnCount: _root_.scala.Option[Int],
      override val _passthroughFields: immutable$Map[Short, TFieldBlob])
    extends TRowSet {
    def this(
      startRowOffset: Long,
      rows: _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow] = _root_.scala.collection.immutable.Nil,
      columns: _root_.scala.Option[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]] = _root_.scala.None,
      binaryColumns: _root_.scala.Option[_root_.java.nio.ByteBuffer] = _root_.scala.None,
      columnCount: _root_.scala.Option[Int] = _root_.scala.None
    ) = this(
      startRowOffset,
      rows,
      columns,
      binaryColumns,
      columnCount,
      immutable$Map.empty[Short, TFieldBlob]
    )
  }

  /**
   * This is another Immutable, this however keeps strings as lazy values that are lazily decoded from the backing
   * array byte on read.
   */
  private[this] class LazyImmutable(
      _proto: LazyTProtocol,
      _buf: Array[Byte],
      _start_offset: Int,
      _end_offset: Int,
      val startRowOffset: Long,
      val rows: _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow],
      val columns: _root_.scala.Option[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]],
      val binaryColumns: _root_.scala.Option[_root_.java.nio.ByteBuffer],
      columnCountOffset: Int,
      override val _passthroughFields: immutable$Map[Short, TFieldBlob])
    extends TRowSet {

    override def write(_oprot: TProtocol): Unit = {
      _oprot match {
        case i: LazyTProtocol => i.writeRaw(_buf, _start_offset, _end_offset - _start_offset)
        case _ => super.write(_oprot)
      }
    }

    lazy val columnCount: _root_.scala.Option[Int] =
      if (columnCountOffset == -1)
        None
      else {
        Some(_proto.decodeI32(_buf, columnCountOffset))
      }

    /**
     * Override the super hash code to make it a lazy val rather than def.
     *
     * Calculating the hash code can be expensive, caching it where possible
     * can provide significant performance wins. (Key in a hash map for instance)
     * Usually not safe since the normal constructor will accept a mutable map or
     * set as an arg
     * Here however we control how the class is generated from serialized data.
     * With the class private and the contract that we throw away our mutable references
     * having the hash code lazy here is safe.
     */
    override lazy val hashCode = super.hashCode
  }

  /**
   * This Proxy trait allows you to extend the TRowSet trait with additional state or
   * behavior and implement the read-only methods from TRowSet using an underlying
   * instance.
   */
  trait Proxy extends TRowSet {
    protected def _underlying_TRowSet: TRowSet
    override def startRowOffset: Long = _underlying_TRowSet.startRowOffset
    override def rows: _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow] = _underlying_TRowSet.rows
    override def columns: _root_.scala.Option[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]] = _underlying_TRowSet.columns
    override def binaryColumns: _root_.scala.Option[_root_.java.nio.ByteBuffer] = _underlying_TRowSet.binaryColumns
    override def columnCount: _root_.scala.Option[Int] = _underlying_TRowSet.columnCount
    override def _passthroughFields: immutable$Map[Short, TFieldBlob] = _underlying_TRowSet._passthroughFields
  }
}

/**
 * Prefer the companion object's [[org.apache.hive.service.rpc.thrift.TRowSet.apply]]
 * for construction if you don't need to specify passthrough fields.
 */
trait TRowSet
  extends ThriftStruct
  with _root_.scala.Product5[Long, _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow], Option[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]], Option[_root_.java.nio.ByteBuffer], Option[Int]]
  with ValidatingThriftStruct[TRowSet]
  with java.io.Serializable
{
  import TRowSet._

  def startRowOffset: Long
  def rows: _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow]
  def columns: _root_.scala.Option[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]]
  def binaryColumns: _root_.scala.Option[_root_.java.nio.ByteBuffer]
  def columnCount: _root_.scala.Option[Int]

  def _passthroughFields: immutable$Map[Short, TFieldBlob] = immutable$Map.empty

  def _1: Long = startRowOffset
  def _2: _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow] = rows
  def _3: _root_.scala.Option[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]] = columns
  def _4: _root_.scala.Option[_root_.java.nio.ByteBuffer] = binaryColumns
  def _5: _root_.scala.Option[Int] = columnCount

  def toTuple: _root_.scala.Tuple5[Long, _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow], Option[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]], Option[_root_.java.nio.ByteBuffer], Option[Int]] = {
    (
      startRowOffset,
      rows,
      columns,
      binaryColumns,
      columnCount
    )
  }


  /**
   * Gets a field value encoded as a binary blob using TCompactProtocol.  If the specified field
   * is present in the passthrough map, that value is returned.  Otherwise, if the specified field
   * is known and not optional and set to None, then the field is serialized and returned.
   */
  def getFieldBlob(_fieldId: Short): _root_.scala.Option[TFieldBlob] = {
    lazy val _buff = new TMemoryBuffer(32)
    lazy val _oprot = new TCompactProtocol(_buff)
    _passthroughFields.get(_fieldId) match {
      case blob: _root_.scala.Some[TFieldBlob] => blob
      case _root_.scala.None => {
        val _fieldOpt: _root_.scala.Option[TField] =
          _fieldId match {
            case 1 =>
              if (true) {
                writeStartRowOffsetValue(startRowOffset, _oprot)
                _root_.scala.Some(TRowSet.StartRowOffsetField)
              } else {
                _root_.scala.None
              }
            case 2 =>
              if (rows ne null) {
                writeRowsValue(rows, _oprot)
                _root_.scala.Some(TRowSet.RowsField)
              } else {
                _root_.scala.None
              }
            case 3 =>
              if (columns.isDefined) {
                writeColumnsValue(columns.get, _oprot)
                _root_.scala.Some(TRowSet.ColumnsField)
              } else {
                _root_.scala.None
              }
            case 4 =>
              if (binaryColumns.isDefined) {
                writeBinaryColumnsValue(binaryColumns.get, _oprot)
                _root_.scala.Some(TRowSet.BinaryColumnsField)
              } else {
                _root_.scala.None
              }
            case 5 =>
              if (columnCount.isDefined) {
                writeColumnCountValue(columnCount.get, _oprot)
                _root_.scala.Some(TRowSet.ColumnCountField)
              } else {
                _root_.scala.None
              }
            case _ => _root_.scala.None
          }
        _fieldOpt match {
          case _root_.scala.Some(_field) =>
            _root_.scala.Some(TFieldBlob(_field, Buf.ByteArray.Owned(_buff.getArray())))
          case _root_.scala.None =>
            _root_.scala.None
        }
      }
    }
  }

  /**
   * Collects TCompactProtocol-encoded field values according to `getFieldBlob` into a map.
   */
  def getFieldBlobs(ids: TraversableOnce[Short]): immutable$Map[Short, TFieldBlob] =
    (ids flatMap { id => getFieldBlob(id) map { id -> _ } }).toMap

  /**
   * Sets a field using a TCompactProtocol-encoded binary blob.  If the field is a known
   * field, the blob is decoded and the field is set to the decoded value.  If the field
   * is unknown and passthrough fields are enabled, then the blob will be stored in
   * _passthroughFields.
   */
  def setField(_blob: TFieldBlob): TRowSet = {
    var startRowOffset: Long = this.startRowOffset
    var rows: _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow] = this.rows
    var columns: _root_.scala.Option[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]] = this.columns
    var binaryColumns: _root_.scala.Option[_root_.java.nio.ByteBuffer] = this.binaryColumns
    var columnCount: _root_.scala.Option[Int] = this.columnCount
    var _passthroughFields = this._passthroughFields
    _blob.id match {
      case 1 =>
        startRowOffset = readStartRowOffsetValue(_blob.read)
      case 2 =>
        rows = readRowsValue(_blob.read)
      case 3 =>
        columns = _root_.scala.Some(readColumnsValue(_blob.read))
      case 4 =>
        binaryColumns = _root_.scala.Some(readBinaryColumnsValue(_blob.read))
      case 5 =>
        columnCount = _root_.scala.Some(readColumnCountValue(_blob.read))
      case _ => _passthroughFields += (_blob.id -> _blob)
    }
    new Immutable(
      startRowOffset,
      rows,
      columns,
      binaryColumns,
      columnCount,
      _passthroughFields
    )
  }

  /**
   * If the specified field is optional, it is set to None.  Otherwise, if the field is
   * known, it is reverted to its default value; if the field is unknown, it is removed
   * from the passthroughFields map, if present.
   */
  def unsetField(_fieldId: Short): TRowSet = {
    var startRowOffset: Long = this.startRowOffset
    var rows: _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow] = this.rows
    var columns: _root_.scala.Option[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]] = this.columns
    var binaryColumns: _root_.scala.Option[_root_.java.nio.ByteBuffer] = this.binaryColumns
    var columnCount: _root_.scala.Option[Int] = this.columnCount

    _fieldId match {
      case 1 =>
        startRowOffset = 0L
      case 2 =>
        rows = _root_.scala.collection.immutable.Nil
      case 3 =>
        columns = _root_.scala.None
      case 4 =>
        binaryColumns = _root_.scala.None
      case 5 =>
        columnCount = _root_.scala.None
      case _ =>
    }
    new Immutable(
      startRowOffset,
      rows,
      columns,
      binaryColumns,
      columnCount,
      _passthroughFields - _fieldId
    )
  }

  /**
   * If the specified field is optional, it is set to None.  Otherwise, if the field is
   * known, it is reverted to its default value; if the field is unknown, it is removed
   * from the passthroughFields map, if present.
   */
  def unsetStartRowOffset: TRowSet = unsetField(1)

  def unsetRows: TRowSet = unsetField(2)

  def unsetColumns: TRowSet = unsetField(3)

  def unsetBinaryColumns: TRowSet = unsetField(4)

  def unsetColumnCount: TRowSet = unsetField(5)


  override def write(_oprot: TProtocol): Unit = {
    TRowSet.validate(this)
    _oprot.writeStructBegin(Struct)
    writeStartRowOffsetField(startRowOffset, _oprot)
    if (rows ne null) writeRowsField(rows, _oprot)
    if (columns.isDefined) writeColumnsField(columns.get, _oprot)
    if (binaryColumns.isDefined) writeBinaryColumnsField(binaryColumns.get, _oprot)
    if (columnCount.isDefined) writeColumnCountField(columnCount.get, _oprot)
    if (_passthroughFields.nonEmpty) {
      _passthroughFields.values.foreach { _.write(_oprot) }
    }
    _oprot.writeFieldStop()
    _oprot.writeStructEnd()
  }

  def copy(
    startRowOffset: Long = this.startRowOffset,
    rows: _root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow] = this.rows,
    columns: _root_.scala.Option[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]] = this.columns,
    binaryColumns: _root_.scala.Option[_root_.java.nio.ByteBuffer] = this.binaryColumns,
    columnCount: _root_.scala.Option[Int] = this.columnCount,
    _passthroughFields: immutable$Map[Short, TFieldBlob] = this._passthroughFields
  ): TRowSet =
    new Immutable(
      startRowOffset,
      rows,
      columns,
      binaryColumns,
      columnCount,
      _passthroughFields
    )

  override def canEqual(other: Any): Boolean = other.isInstanceOf[TRowSet]

  private def _equals(x: TRowSet, y: TRowSet): Boolean =
      x.productArity == y.productArity &&
      x.productIterator.sameElements(y.productIterator) &&
      x._passthroughFields == y._passthroughFields

  override def equals(other: Any): Boolean =
    canEqual(other) &&
      _equals(this, other.asInstanceOf[TRowSet])

  override def hashCode: Int = {
    _root_.scala.runtime.ScalaRunTime._hashCode(this)
  }

  override def toString: String = _root_.scala.runtime.ScalaRunTime._toString(this)


  override def productArity: Int = 5

  override def productElement(n: Int): Any = n match {
    case 0 => this.startRowOffset
    case 1 => this.rows
    case 2 => this.columns
    case 3 => this.binaryColumns
    case 4 => this.columnCount
    case _ => throw new IndexOutOfBoundsException(n.toString)
  }

  override def productPrefix: String = "TRowSet"

  def _codec: ValidatingThriftStructCodec3[TRowSet] = TRowSet

  def newBuilder(): StructBuilder[TRowSet] = new TRowSetStructBuilder(_root_.scala.Some(this), fieldTypes)
}

private[thrift] class TRowSetStructBuilder(instance: _root_.scala.Option[TRowSet], fieldTypes: IndexedSeq[ClassTag[_]])
    extends StructBuilder[TRowSet](fieldTypes) {

  def build(): TRowSet = instance match {
    case _root_.scala.Some(i) =>
      TRowSet(
        (if (fieldArray(0) == null) i.startRowOffset else fieldArray(0)).asInstanceOf[Long],
        (if (fieldArray(1) == null) i.rows else fieldArray(1)).asInstanceOf[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow]],
        (if (fieldArray(2) == null) i.columns else fieldArray(2)).asInstanceOf[_root_.scala.Option[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]]],
        (if (fieldArray(3) == null) i.binaryColumns else fieldArray(3)).asInstanceOf[_root_.scala.Option[_root_.java.nio.ByteBuffer]],
        (if (fieldArray(4) == null) i.columnCount else fieldArray(4)).asInstanceOf[_root_.scala.Option[Int]]
      )
    case _root_.scala.None =>
      if (fieldArray.contains(null)) throw new InvalidFieldsException(structBuildError("TRowSet"))
      else {
        TRowSet(
          fieldArray(0).asInstanceOf[Long],
          fieldArray(1).asInstanceOf[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TRow]],
          fieldArray(2).asInstanceOf[_root_.scala.Option[_root_.scala.collection.Seq[org.apache.hive.service.rpc.thrift.TColumn]]],
          fieldArray(3).asInstanceOf[_root_.scala.Option[_root_.java.nio.ByteBuffer]],
          fieldArray(4).asInstanceOf[_root_.scala.Option[Int]]
        )
      }
    }
}

